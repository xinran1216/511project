{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8675, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"mbti_1.csv\")\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4080    'I loved All the Light We Cannot See by Anthon...\n",
       "2614    'It depends. If I care about it, I fight and g...\n",
       "5414    'Welcome home, sonny :laughing:|||Just because...\n",
       "1039    That's really cool of you. I like it when anyo...\n",
       "8294    'The duck is named Zeus.|||http://www.youtube....\n",
       "                              ...                        \n",
       "5734    'I have 2 cats and a chihuahua/pug mix. It's r...\n",
       "5191    Ever since I can remember I have suffered/live...\n",
       "5390    'I've known a couple of INFJ guys and they see...\n",
       "860     'Even the loner gets lonely. I feel like it's ...\n",
       "7270    Before reading the responses to this thread, I...\n",
       "Name: posts, Length: 6940, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['posts'], data['type'], test_size=0.2, random_state=42)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhangxinran/opt/miniconda3/envs/ANlY501/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "============================= Pipeline Overview =============================\u001b[0m\n",
      "\n",
      "#   Component         Assigns               Requires   Scores             Retokenizes\n",
      "-   ---------------   -------------------   --------   ----------------   -----------\n",
      "0   tok2vec           doc.tensor                                          False      \n",
      "                                                                                     \n",
      "1   tagger            token.tag                        tag_acc            False      \n",
      "                                                                                     \n",
      "2   parser            token.dep                        dep_uas            False      \n",
      "                      token.head                       dep_las                       \n",
      "                      token.is_sent_start              dep_las_per_type              \n",
      "                      doc.sents                        sents_p                       \n",
      "                                                       sents_r                       \n",
      "                                                       sents_f                       \n",
      "                                                                                     \n",
      "3   attribute_ruler                                                       False      \n",
      "                                                                                     \n",
      "4   lemmatizer        token.lemma                      lemma_acc          False      \n",
      "                                                                                     \n",
      "5   ner               doc.ents                         ents_f             False      \n",
      "                      token.ent_iob                    ents_p                        \n",
      "                      token.ent_type                   ents_r                        \n",
      "                                                       ents_per_type                 \n",
      "                                                                                     \n",
      "6   ng20                                                                  False      \n",
      "\n",
      "\u001b[38;5;2mâœ” No problems found.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "\n",
    "\n",
    "pipeline = spacy.load('en_core_web_sm')\n",
    "\n",
    "# http://emailregex.com/\n",
    "email_re = r\"\"\"(?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*|\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])\"\"\"\n",
    "\n",
    "# replace = [ (pattern-to-replace, replacement),  ...]\n",
    "replace = [\n",
    "    (r\"(http:).*?(\\|\\|\\|)\",r\"\\1\"),\n",
    "    (r\"]*>(.*?)\", r\"\\1\"),  # Matches most URLs\n",
    "    (email_re, \"email\"),            # Matches emails\n",
    "    (r\"(?<=\\d),(?=\\d)\", \"\"),        # Remove commas in numbers\n",
    "    (r\"\\d+\", \"number\"),              # Map digits to special token \n",
    "    (r\"[\\t\\n\\r\\*\\.\\@\\,\\-\\/]\", \" \"), # Punctuation and other junk\n",
    "    (r\"\\s+\", \" \"),                   # Stips extra whitespace\n",
    "    (r\"http:\",r\" \"),\n",
    "    (r\"\\|\\|\\|\",\" \"),\n",
    "    (r\"(https:).*?(\\|\\|\\|)\",r\"\\1\")\n",
    "]\n",
    "\n",
    "train_sentences = []\n",
    "test_sentences = []\n",
    "for i, d in enumerate(X_train):\n",
    "    for repl in replace:\n",
    "        d = re.sub(repl[0], repl[1], d)\n",
    "    train_sentences.append(d)\n",
    "for i, d in enumerate(X_test):\n",
    "    for repl in replace:\n",
    "        d = re.sub(repl[0], repl[1], d)\n",
    "    test_sentences.append(d)\n",
    "\n",
    "@Language.component(\"ng20\")\n",
    "def ng20_preprocess(doc):\n",
    "    tokens = [token for token in doc \n",
    "                if not any((token.is_stop, token.is_punct))]\n",
    "    tokens = [token.lemma_.lower().strip() for token in tokens]\n",
    "    tokens = [token for token in tokens if token]\n",
    "    return \" \".join(tokens)\n",
    "pipeline.add_pipe(\"ng20\");\n",
    "pipeline.analyze_pipes(pretty=True)\n",
    "\n",
    "docs = [pipeline(d) for d in train_sentences]\n",
    "test_docs = [pipeline(d) for d in test_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(docs).to_csv(\"train_posts.csv\",index=None)\n",
    "pd.DataFrame(test_docs).to_csv(\"test_posts.csv\",index=None)\n",
    "pd.DataFrame(y_train).to_csv(\"train_type.csv\",index=None)\n",
    "pd.DataFrame(y_test).to_csv(\"test_type.csv\",index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data is highly imbalanced, we break down the problem into building four classifiers to classify eight types of personality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "P    4175\n",
       "J    2765\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label1 = y_train.apply(lambda x:x[0])\n",
    "train_label2 = y_train.apply(lambda x:x[1])\n",
    "train_label3 = y_train.apply(lambda x:x[2])\n",
    "train_label4 = y_train.apply(lambda x:x[3])\n",
    "\n",
    "test_label1 = y_test.apply(lambda x:x[0])\n",
    "test_label2 = y_test.apply(lambda x:x[1])\n",
    "test_label3 = y_test.apply(lambda x:x[2])\n",
    "test_label4 = y_test.apply(lambda x:x[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_label3 = train_label3.unique()\n",
    "label_dict3 = {}\n",
    "for index, possible_label in enumerate(possible_label3):\n",
    "    label_dict3[possible_label] = index\n",
    "train_label3 = train_label3.replace(label_dict3)\n",
    "test_label3 = test_label3.replace(label_dict3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('ANlY501')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c9e2434c64481e99fb3d6ef90450d187aacbc4d5a2826403eda8739c51f88b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
